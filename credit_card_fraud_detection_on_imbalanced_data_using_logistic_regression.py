# -*- coding: utf-8 -*-
"""Credit Card Fraud Detection on Imbalanced Data Using Logistic Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tl9RfLe-d44nxsiyAGTJW02VnOXpTUXl

Importing libraries and dependencies
"""

import numpy as np  # for numpy arrays manupulation
import pandas as pd  #for loading data set and data analysis
from sklearn.model_selection import train_test_split  #for spliting data into training and testing parts
from sklearn.linear_model import LogisticRegression  # importing model for classification
from sklearn.metrics import accuracy_score  #metrics to evaluate model

"""Data loading and preprocessing"""

df= pd.read_csv("/content/creditcard.csv")  #providing dataset file path to create data frame

df.head()  # first 5 rows displaying

# we have limited access to this sensative data,the time feature represents time interval between different transaction starting from first transaction at 0

# all other features are converted into numerical values describe by v1,v2,v3... these are converted through principal component analysis,the idea is simple that the info is sensative so complete info cant be provided

# thes last features represents amount in usd and class labels

# 0 represents ligit or normal transaction,1 represent fraudulent transaction

df.tail()  #displaying last five rows

df.info()  #statistical infor to check data type of features

df.isnull().sum()  #checking missing values

df = df.dropna()  # we will drop missing values as these are very less in number

df.isnull().sum()  #checking missing values again

df['Class'].value_counts()  #counting no. of categories in feature "class"

# separating data for analysis on the basis of "class" categories

legit=df[df.Class==0] #storing in variable "legit to show legit transactions"
fraud=df[df.Class==1] #storing in variable "fraud" to show fraudulent transactions

print(legit.shape) # rows and columns checking
print(fraud.shape)

# statiscal analysis of  legit class based on amount

df['Amount']= df['Amount'].astype(float) # changing the data type of feature "Amount" from object to float

legit.Amount.describe(include='all') #descriptive analysis of legit transactions.count shows total no. of transactions,mean shows mean transactional  amount is 88 dollar

#Descriptive info of fraud class on the basis of amount

fraud.Amount.describe(include='all') # total 492 fraud transactions, with mean fraud transactional amouunt id 122 US dollars

# comparing all features on the basis of mean

df.groupby('Class').mean() # this will group by every values legit and fraudulent class on the basis of mean for and compare it with every feature

# legit transactional mean amount is 88 dollars while fraudulent mean transactional amount is 122 dollars

"""Treating Imbalanced data by using upsampling and down sampling techniques.

Under sampling
"""

# we will build a sample dataset representing the similar distribution of normal legit transaction and fradulent transactions

df.shape # total rows and columns

legit.shape  #legit transactions have 284315 data points or total number while having 31 features

fraud.shape # total 492 datapoint of fraud transaction or total no.

# we will take 492 legit transactions sand join it with 492 fraud transaction so we have uniform distribution for better prediction

legit_sample=legit.sample(n=492) # creating sample of legit class to under sample and create similar normal distribution as same as fraudulen class, n represent no. of samples to be created

"""Concatenating two dataframe that is legit and fraud"""

new_dataset= pd.concat([legit_sample,fraud],axis=0)  # pandas concatenation function two join two data frame,axis=0 represents the joining will be row wise

new_dataset.head() #showing first 5 rows

new_dataset.shape # new data set after merging have 984 rows and 31 columns

new_dataset.tail() #last 5 rows

# the serial no. shows that concatenation is dont randomly

new_dataset['Class'].value_counts() #counting legit and fraudulent class values in new merge data frame

new_dataset.groupby('Class').mean() #comparing by group by on new merge data frame

# comparing categories through through mean using group by,we come to know that mean of earlier data frame and sampled merge data frame is not very 3different,that shows random sampling is done properly in a good way

"""Separating  features and target"""

x=new_dataset.drop(columns='Class',axis=1)  # droping target feature from df and storing in x
y=new_dataset['Class']  # y contains target feature values

print(x) #printing data
print(y)

"""Train test split"""

X_train,X_test,Y_train,Y_test=train_test_split(x,y,test_size=0.2,stratify=y,random_state=2)# test size is 20 percent,80 percent data for training,stratify represents that the split will be according to the distribution in target feature,random_state assure similar sampling if model run twice aor thrice or if model runs on two different machines

print(X_train.shape)  #printing rows and columns
print(X_test.shape)
print(Y_train.shape)
print(Y_test.shape)

"""Model training"""

model= LogisticRegression()  #storing model in variable model
model.fit(X_train,Y_train) #training training data and labels on model

"""Model Evaluation"""

# training data evaluation

X_train_prediction= model.predict(X_train) #predicting model on train data
X_train_accuracy= accuracy_score(X_train_prediction,Y_train) #comparing prediction and associated labels through metrics
prediction=X_train_accuracy #storing accuracy score in variable prediction
print(prediction) #printing accuracy of training data,it is usefull because it tells  through difference if the model is over fitted or not

#test data evaluation

X_test_prediction= model.predict(X_test)
X_test_accuracy= accuracy_score(X_test_prediction,Y_test)
prediction=X_test_accuracy
print(prediction)  #printing testing accuracy